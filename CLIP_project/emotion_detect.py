import torch
print("CUDA available:", torch.cuda.is_available())
print("Device count:", torch.cuda.device_count())
print("Current device:", torch.cuda.current_device() if torch.cuda.is_available() else "None")
# !nvidia-smi

import cv2
import torch
import clip
import numpy as np
from PIL import Image
from typing import List
import sys
import os

# è®¾å¤‡é€‰æ‹©
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Using device:", device)

# åŠ è½½æ¨¡å‹å’Œé¢„å¤„ç†å™¨
model, preprocess = clip.load("ViT-B/32", device=device)

model.float()

# æƒ…ç»ªæç¤ºè¯
emotion_labels = [
    # å–œ Joy
    "Joy: a vibrant scene with warm sunlight, blooming flowers, bright colors, and a peaceful landscape that evokes happiness.A dynamic and energetic scene full of movement, bright lights, fireworks, and visual excitement like a festival or concert",

    # æ€’ Anger
    "Anger: a dramatic and chaotic environment with dark clouds, aggressive fire, broken structures, and intense tension in the atmosphere",

    # å“€ Sadness
    "Sadness: a lonely and desaturated scene of a rainy day, empty streets, or a quiet foggy forest that feels melancholic and heavy",

    # å®³æ€• Fear
    "Fear: a dark and eerie environment with shadows, fog, abandoned buildings, or looming shapes that create a strong sense of danger, suspense, or unease",

    "Excited: A fast-paced and thrilling electronic track with intense beats and energetic drops, reflecting the excitement and adrenaline of competitive gameplay."

    # å®é™ Inner Peace
    "Inner Peace: a calm and tranquil natural scene with soft sunlight, still water, open space, gentle hills or mountains, and a feeling of deep serenity"
]
emotion_list = ["Joy", "Anger", "Sadness", "Fear", "Excited", "Inner Peace"]
text_prompts = [f"a scene evoking {e}" for e in emotion_labels]
text_tokens = clip.tokenize(text_prompts).to(device)

def extract_frames(video_path: str, interval_sec: float = 1.0) -> List[Image.Image]:
    """
    æ¯ interval_sec æˆªå–ä¸€å¸§ï¼Œè¿”å› PIL å›¾åƒåˆ—è¡¨
    """
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_interval = int(fps * interval_sec)
    frames = []

    frame_idx = 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if frame_idx % frame_interval == 0:
            image_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            frames.append(image_pil)
        frame_idx += 1

    cap.release()
    return frames


def predict_emotion_for_images(images: List[Image.Image], model, preprocess, text_features, device="cpu") -> np.ndarray:
    """
    å¯¹å›¾åƒåˆ—è¡¨è¿›è¡Œæƒ…ç»ªè¯†åˆ«ï¼Œè¿”å›æ¯å¸§æƒ…ç»ªå¾—åˆ†ç»„æˆçš„æ•°ç»„ shape=[num_frames, num_emotions]
    """
    all_probs = []
    with torch.no_grad():
        for img in images:
            image_input = preprocess(img).unsqueeze(0).to(device)
            image_features = model.encode_image(image_input)
            logits_per_image = (image_features @ text_features.T).softmax(dim=-1)
            all_probs.append(logits_per_image.cpu().numpy()[0])
    return np.array(all_probs)


def analyze_video_emotion(video_path: str, video_name: str, model, preprocess, device="cpu") -> None:
    """
    ä¸»å‡½æ•°ï¼šè¾“å…¥è§†é¢‘è·¯å¾„ï¼Œè¾“å‡ºåˆ†æç»“æœ
    """
    print(f"â³ æ­£åœ¨å¤„ç†è§†é¢‘: {video_path}")
    # 1. ç¼–ç æ–‡æœ¬ç‰¹å¾ï¼ˆåªåšä¸€æ¬¡ï¼‰
    text_tokens = clip.tokenize(text_prompts).to(device)
    with torch.no_grad():
        text_features = model.encode_text(text_tokens)

    # 2. æå–å¸§
    frames = extract_frames(video_path, interval_sec=1.0)
    print(f"âœ… æå–å¸§æ•°: {len(frames)}")

    if not frames:
        print("âš ï¸ æœªæå–åˆ°å¸§ï¼Œæ£€æŸ¥è§†é¢‘æ–‡ä»¶æ˜¯å¦æœ‰æ•ˆ")
        return ''

    # 3. å›¾åƒé¢„æµ‹
    all_probs = predict_emotion_for_images(frames, model, preprocess, text_features, device=device)

    # 4. æ±‡æ€»åˆ†æ
    avg_probs = all_probs.mean(axis=0)

    # 5. è¾“å‡º
    print("\nğŸ“Š è§†é¢‘æƒ…ç»ªåˆ†å¸ƒï¼š")
    for emotion, score in zip(emotion_labels, avg_probs):
        # åªæ‰“å°ç¬¬ä¸€ä¸ªå†’å·å‰é¢çš„æƒ…æ„Ÿåç§°
        emotion_name = emotion.split(":")[0]
        print(f"{emotion_name:<15}: {score:.3f}")

    print("\nğŸŒŸ æ¨æµ‹è§†é¢‘æƒ…æ„ŸåŸºè°ƒï¼š", emotion_labels[np.argmax(avg_probs)].split(":")[0])

    target_emotion = emotion_labels[np.argmax(avg_probs)].split(":")[0]
    index = emotion_list.index(target_emotion)
    description = emotion_labels[index]     # é€šè¿‡ç´¢å¼•è·å– emotion_labels ä¸­çš„å¯¹åº”æè¿°

    current_path = os.path.dirname(os.path.abspath(__file__))
    output_folder = os.path.join(current_path, "analysis_output", video_name)  # æ–°å»ºå­æ–‡ä»¶å¤¹
    os.makedirs(output_folder, exist_ok=True)  # åˆ›å»ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
    output_path = os.path.join(current_path, "analysis_output", video_name, "clip_emotion_analysis.txt")
    with open(output_path, "w", ) as f:
        f.write(description)
        f.close()


# ğŸ§  å…¥å£ï¼šä»å‘½ä»¤è¡Œè·å–è§†é¢‘åå­æ–‡ä»¶å¤¹
if __name__ == "__main__":
    if len(sys.argv) != 3:
        print("æœªæ‰¾åˆ°æ­£ç¡®çš„éŸ³ä¹è·¯å¾„")
        sys.exit(1)

    # ä»å‘½ä»¤è¡Œå‚æ•°è·å–è¾“å…¥
    music_path = sys.argv[1]
    video_name = sys.argv[2]

    analyze_video_emotion(music_path, video_name, model, preprocess, device=device)