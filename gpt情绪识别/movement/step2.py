import os
import cv2
import torch
import pickle
import numpy as np
from tqdm import tqdm
from PIL import Image
import clip  # ä½¿ç”¨è‹±æ–‡ç‰ˆ OpenAI CLIP

# ---------- é…ç½® ----------
video_path = "./movement/test.mp4"                # â† ä¿®æ”¹ä¸ºä½ çš„è§†é¢‘è·¯å¾„
output_dir = "lmske_intermediate"            # â† ä¸­é—´ç»“æœè¾“å‡ºè·¯å¾„
device = "cuda" if torch.cuda.is_available() else "cpu"

# ---------- åˆå§‹åŒ– ----------
os.makedirs(output_dir, exist_ok=True)
print("ğŸš€ æ­£åœ¨åŠ è½½è‹±æ–‡ CLIP æ¨¡å‹...")
model, preprocess = clip.load("ViT-B/32", device=device)

# ---------- æå–ç‰¹å¾ ----------
def extract_clip_features(video_path):
    print("ğŸ§  å¼€å§‹æå–è§†é¢‘å¸§è¯­ä¹‰ç‰¹å¾ï¼ˆè‹±æ–‡ CLIPï¼‰...")
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    features = []
    frame_indices = []

    for i in tqdm(range(total_frames), desc="æå–ä¸­"):
        ret, frame = cap.read()
        if not ret:
            break

        # è½¬æ¢ä¸º RGB å›¾åƒ
        img = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        img_pre = preprocess(img).unsqueeze(0).to(device)

        with torch.no_grad():
            embedding = model.encode_image(img_pre).cpu().numpy().squeeze()

        features.append(embedding)
        frame_indices.append(i)

    cap.release()

    # ä¿å­˜ç»“æœ
    np.save(os.path.join(output_dir, "features.npy"), np.array(features))
    with open(os.path.join(output_dir, "frames_list.pkl"), "wb") as f:
        pickle.dump(frame_indices, f)

    print(f"âœ… æå–å®Œæˆï¼š{len(features)} å¸§ï¼Œç»“æœä¿å­˜åœ¨ï¼š{output_dir}/")

# ---------- æ‰§è¡Œ ----------
if __name__ == "__main__":
    if not os.path.exists(video_path):
        print("âŒ è§†é¢‘è·¯å¾„ä¸å­˜åœ¨ï¼Œè¯·æ£€æŸ¥ video_path")
    else:
        extract_clip_features(video_path)
